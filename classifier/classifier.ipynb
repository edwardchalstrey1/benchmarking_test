{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our algorithm locally\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_report, local_results = classifier.results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our algorithm in docker container after building\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  26.62kB\r",
      "\r\n",
      "Step 1/8 : FROM python:3\n",
      " ---> ac069ebfe1e1\n",
      "Step 2/8 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 5a84d23aa7b5\n",
      "Step 3/8 : RUN pip3 install numpy\n",
      " ---> Using cache\n",
      " ---> 4383ac463a3b\n",
      "Step 4/8 : RUN pip3 install scipy\n",
      " ---> Using cache\n",
      " ---> 6fa2c9da864b\n",
      "Step 5/8 : RUN pip3 install scikit-learn\n",
      " ---> Using cache\n",
      " ---> 0b888dbaed11\n",
      "Step 6/8 : COPY classifier.py /classifier.py\n",
      " ---> e0320dddac2f\n",
      "Step 7/8 : COPY display_classifier_results.py /display_classifier_results.py\n",
      " ---> 8f49ac61d708\n",
      "Step 8/8 : CMD python3 display_classifier_results.py\n",
      " ---> Running in 65fced0f8fd5\n",
      "Removing intermediate container 65fced0f8fd5\n",
      " ---> 9123aecf8967\n",
      "Successfully built 9123aecf8967\n",
      "Successfully tagged edwardchalstrey/classifier:latest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build -t edwardchalstrey/classifier:latest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out docker_results\n",
    "docker run edwardchalstrey/classifier:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_results = ast.literal_eval(docker_results)\n",
    "docker_report, docker_results = docker_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do they compare?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Version  </td><td>Training time (s)  </td><td>Prediction time (s) </td><td>Performance (micro avg f1 score)</td></tr>\n",
       "<tr><td>Basic    </td><td>0.0927119255065918 </td><td>0.0359799861907959  </td><td>0.9566184649610678              </td></tr>\n",
       "<tr><td>Container</td><td>0.11895322799682617</td><td>0.050348520278930664</td><td>0.9566184649610678              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "headers = [\"Version\"]\n",
    "c_results = [\"Basic\"]\n",
    "d_results = [\"Container\"]\n",
    "for k, v in local_results.items():\n",
    "    headers.append(k)\n",
    "    c_results.append(v)\n",
    "for k, v in docker_results.items():\n",
    "    d_results.append(v)\n",
    "display(HTML(tabulate.tabulate([headers, c_results, d_results], tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        88\n",
      "           1       1.00      0.98      0.99        91\n",
      "           2       0.99      0.97      0.98        86\n",
      "           3       0.99      0.85      0.91        91\n",
      "           4       0.99      0.92      0.96        92\n",
      "           5       0.95      0.96      0.95        91\n",
      "           6       0.99      0.99      0.99        91\n",
      "           7       0.98      0.98      0.98        89\n",
      "           8       0.86      1.00      0.93        88\n",
      "           9       0.86      0.97      0.91        92\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       899\n",
      "   macro avg       0.96      0.96      0.96       899\n",
      "weighted avg       0.96      0.96      0.96       899\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        88\n",
      "           1       1.00      0.98      0.99        91\n",
      "           2       0.99      0.97      0.98        86\n",
      "           3       0.99      0.85      0.91        91\n",
      "           4       0.99      0.92      0.96        92\n",
      "           5       0.95      0.96      0.95        91\n",
      "           6       0.99      0.99      0.99        91\n",
      "           7       0.98      0.98      0.98        89\n",
      "           8       0.86      1.00      0.93        88\n",
      "           9       0.86      0.97      0.91        92\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       899\n",
      "   macro avg       0.96      0.96      0.96       899\n",
      "weighted avg       0.96      0.96      0.96       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(local_report)\n",
    "print(docker_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same classification algorithm, different dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#digits = datasets.load_digits()\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_to_fit_from = iris.target[(n_samples // 6)*1:(n_samples // 6)*2]\n",
    "targets_to_fit_from = np.append(targets_to_fit_from, iris.target[(n_samples // 6)*3:(n_samples // 6)*4])\n",
    "targets_to_fit_from = np.append(targets_to_fit_from, iris.target[(n_samples // 6)*5:(n_samples // 6)*6])\n",
    "\n",
    "data_to_fit_from = iris.data[(n_samples // 6)*1:(n_samples // 6)*2]\n",
    "data_to_fit_from = np.concatenate((data_to_fit_from, iris.data[(n_samples // 6)*3:(n_samples // 6)*4]))\n",
    "data_to_fit_from = np.concatenate((data_to_fit_from, iris.data[(n_samples // 6)*5:(n_samples // 6)*6]))\n",
    "\n",
    "data_to_predict_from = iris.data[:n_samples // 6]\n",
    "data_to_predict_from = np.concatenate((data_to_predict_from, iris.data[(n_samples // 6)*2:(n_samples // 6)*3]))\n",
    "data_to_predict_from = np.concatenate((data_to_predict_from, iris.data[(n_samples // 6)*4:(n_samples // 6)*5]))\n",
    "\n",
    "expected_targets = iris.target[:n_samples // 6]\n",
    "expected_targets = np.concatenate((expected_targets, iris.target[(n_samples // 6)*2:(n_samples // 6)*3]))\n",
    "expected_targets = np.concatenate((expected_targets, iris.target[(n_samples // 6)*4:(n_samples // 6)*5]))\n",
    "expected_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Training time (s)': 0.0007910728454589844, 'Prediction time (s)': 0.0004901885986328125, 'Performance (micro avg f1 score)': 0.9733333333333334}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       0.96      0.96      0.96        25\n",
      "           2       0.96      0.96      0.96        25\n",
      "\n",
      "   micro avg       0.97      0.97      0.97        75\n",
      "   macro avg       0.97      0.97      0.97        75\n",
      "weighted avg       0.97      0.97      0.97        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#data = iris.data\n",
    "\n",
    "expected = expected_targets\n",
    "\n",
    "classifier = svm.SVC(gamma='scale')\n",
    "\n",
    "start = time.time()\n",
    "classifier.fit(data_to_fit_from, targets_to_fit_from)\n",
    "end = time.time()\n",
    "training_time = end - start\n",
    "\n",
    "start = time.time()\n",
    "predicted = classifier.predict(data_to_predict_from)\n",
    "end = time.time()\n",
    "classifier_time = end - start\n",
    "\n",
    "report = metrics.classification_report(expected, predicted, output_dict=True)\n",
    "\n",
    "performance = report['micro avg']['f1-score']\n",
    "print({\"Training time (s)\": training_time, \"Prediction time (s)\": classifier_time,\n",
    "    \"Performance (micro avg f1 score)\": report['micro avg']['f1-score']})\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
